{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本分词和特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对文本做数据分析时，一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同。\n",
    "\n",
    "中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点：\n",
    "- 中文文本没有像英文那样使用空格隔开的，因此不能直接像英文那样可以直接用最简单的空格和标点符号来完成分词。中文文本一般需要使用分词算法来完成分词。常用的中文分词软件有很多，推荐结巴分词。\n",
    "- 中文的编码不是utf8，而是unicode。\n",
    "   \n",
    "这里选择《人民的名义》的小说原文作为语料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 加入一些人名和地名\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('京州', True)\n",
    "\n",
    "result = None\n",
    "with open('nlp_test_text_ch0.txt', 'rb') as f:\n",
    "    document = f.read()\n",
    "    document_decode = document.decode('gb2312')\n",
    "    document_cut = jieba.cut(document_decode)\n",
    "    result = ' '.join(document_cut)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取\n",
    "中文文本中有很多无效的词，比如“着”，“和”，还有一些标点符号等。我们需要去掉这些停用词。常用的中文停用词表是1208个，文件名stop_words.txt。\n",
    "然后我们就可以用scikit-learn来对文本进行特征提取了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stopwords = []\n",
    "with open(\"stop_words.txt\", 'rb') as f:\n",
    "    stopwords = f.read().decode('gbk').splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.2\n",
      "  (0, 31)\t0.1\n",
      "  (0, 18)\t0.4\n",
      "  (0, 26)\t0.1\n",
      "  (0, 34)\t0.1\n",
      "  (0, 24)\t0.1\n",
      "  (0, 19)\t0.1\n",
      "  (0, 32)\t0.1\n",
      "  (0, 5)\t0.1\n",
      "  (0, 20)\t0.2\n",
      "  (0, 28)\t0.1\n",
      "  (0, 16)\t0.1\n",
      "  (0, 12)\t0.1\n",
      "  (0, 2)\t0.2\n",
      "  (0, 7)\t0.1\n",
      "  (0, 8)\t0.1\n",
      "  (0, 0)\t0.2\n",
      "  (0, 11)\t0.1\n",
      "  (0, 29)\t0.1\n",
      "  (0, 35)\t0.1\n",
      "  (0, 33)\t0.1\n",
      "  (0, 9)\t0.1\n",
      "  (0, 10)\t0.1\n",
      "  (0, 23)\t0.5\n",
      "  (0, 3)\t0.2\n",
      "  (0, 30)\t0.1\n",
      "  (0, 27)\t0.2\n",
      "  (0, 15)\t0.2\n",
      "  (0, 1)\t0.2\n",
      "  (0, 4)\t0.1\n",
      "  (0, 22)\t0.1\n",
      "  (0, 36)\t0.1\n",
      "  (0, 13)\t0.1\n",
      "  (0, 17)\t0.1\n",
      "  (0, 6)\t0.1\n",
      "  (0, 25)\t0.1\n",
      "  (0, 14)\t0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf = vector.fit_transform([result])\n",
    "\n",
    "print( tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 段文本的词语tf-idf权重:\n",
      "一起 0.2\n",
      "万块 0.2\n",
      "三人 0.2\n",
      "下海经商 0.2\n",
      "东挪西撮 0.1\n",
      "事对 0.1\n",
      "以沫 0.1\n",
      "分开 0.1\n",
      "前一晚 0.1\n",
      "县当 0.1\n",
      "县长 0.1\n",
      "喝酒 0.1\n",
      "回忆起 0.1\n",
      "困难 0.1\n",
      "容易 0.1\n",
      "对不起 0.2\n",
      "很大 0.1\n",
      "时期 0.1\n",
      "易学习 0.4\n",
      "有福 0.1\n",
      "李达康 0.2\n",
      "沙瑞金 0.2\n",
      "没想到 0.1\n",
      "王大路 0.5\n",
      "百姓 0.1\n",
      "相助 0.1\n",
      "胸怀 0.1\n",
      "觉得 0.2\n",
      "触动 0.1\n",
      "话别 0.1\n",
      "赔礼道歉 0.1\n",
      "赞叹 0.1\n",
      "这件 0.1\n",
      "道口 0.1\n",
      "金山 0.1\n",
      "降职 0.1\n",
      "风生水 0.1\n"
     ]
    }
   ],
   "source": [
    "#获取词袋模型中的所有词\n",
    "words = vector.get_feature_names()  \n",
    "# tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "weights = tfidf.toarray()  \n",
    "#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重\n",
    "for i in range(len(weights)):\n",
    "    print(\"第\",i,\"段文本的词语tf-idf权重:\")\n",
    "    for j in range(len(words)):  \n",
    "        print(words[j],weights[i][j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
